\section{Manifolds of probability distributions}
In this section, we introduce the differential geometrical description of probability distributions and statistical models. The treatment mainly follows the one of \cite{amariMethodsInformationGeometry2007}, where further details can be found.

\subsection{Space of probability distributions} \label{ch:space-prob-dist}
Consider a \emph{random process} and the set $\mathcal{X}$ of all its possible outcomes. We call this set the \emph{sample space}, and we will only consider random processes for which it is finite. Then, a \emph{probability distribution} on $\mathcal{X}$ is a function $p\in\mathbb{R}^\mathcal{X}\coloneq\{f\mid f\colon \mathcal{X}\to\mathbb{R}\}$ which satisfies
\begin{equation} \label{eq:prob-dist-def}
    p(x)\ge 0 \quad \forall x\in\mathcal{X} \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1
\end{equation}
where $p(x)$ represents the probability of the outcome $x$.

Further, every function $A\in\mathbb{R}^\mathcal{X}$ represents a real \emph{random variable}, as it maps every outcome of a random process to a number. Then the \emph{expectation value} of $A$ when the underlying probability distribution is $p$ is expressed by
\begin{equation}
    \mathrm{E}_p[A]\coloneqq\sum_{x\in\mathcal{X}}p(x)A(x)
\end{equation}
Also, given two random variables $A,B$ their \emph{covariance} is
\begin{equation}
    \mathrm{Cov}_p[A,B]\coloneqq\mathrm{E}_p[(A-\mathrm{E}_p[A])(B-\mathrm{E}_p[B])]
\end{equation}
and so the \emph{variance} of a random variable $A$ is
\begin{equation}
    \mathrm{V}_p[A]\coloneqq\mathrm{Cov}_p[A,A]=\mathrm{E}_p[(A-\mathrm{E}_p[A])^2]
\end{equation}

Let now $N$ be the cardinality of $\mathcal{X}$. To have a picture of $\mathbb{R}^\mathcal{X}$ we can index the outcomes and consider the natural isomorphism between $\mathbb{R}^\mathcal{X}$ and $\mathbb{R}^N$
\begin{equation}
    f \leftrightarrow (f(x_1),\dots,f(x_N))
\end{equation}
then it's easy to recognize that the \emph{space of probability distributions} is a convex subset of the affine subspace $\mathcal{A}_1\coloneqq\{f\in\mathbb{R}^\mathcal{X}\mid\sum_{x\in\mathcal{X}}f(x)=1\}$. In particular, it is the set resulting from the convex mixing of the trivial probability distributions $f_k(x_i)=\delta_{ik}$, represented by the unit vectors of $\mathbb{R}^N$. Finally, it's also interesting to consider the inner product induced on $\mathbb{R}^\mathcal{X}$ by the Euclidean one of $\mathbb{R}^N$. Let $p$ be a probability distribution and $A$ a random variable, then
\begin{equation} \label{eq:inn-prod-Rx}
    p \cdot A = \sum_{x\in\mathcal{X}}p(x)A(x)=\mathrm{E}_p[A]
\end{equation}

\subsection{Statistical models and manifolds} \label{ch:stat-models}
We call an $n$-dimensional \emph{statistical model} on $\mathcal{X}$ a family of probability distributions that are globally parametrized by $n$ real-valued variables. Formally this is a subset $\mathcal{S}$ of the space of probability distributions with an invertible function $\psi\colon\mathcal{S}\to\Xi\subseteq\mathbb{R}^n$, so that we may write
\begin{equation}
    \mathcal{S} = \Bigl\{ p_\xi \bigm\vert \exists\,\xi=(\xi^{(1)},\dots,\xi^{(n)})\in\Xi \colon p_\xi=\psi^{-1}(\xi) \Bigr\}
\end{equation}
where $p_\xi(x)$ may be equivalently written as $p(x;\xi)$ or $p(x;\xi^{(1)},\dots,\xi^{(n)})$. This definition of a statistical model reflects the act of hypothesizing an underlying model, that may depend on some parameters, for the generation of the random variable's samples. Then only a subset, here represented by $\mathcal{S}$, of all the possible probability distributions is considered as a candidate of the underlying probability distribution, and every candidate probability distribution is identified uniquely by the corresponding parameters, here represented by $\xi$.

We now introduce some additional requirements to statistical models so that we may define well-behaved manifolds from them. Firstly we regard $\mathcal{S}$ as a subset of $\mathcal{A}_1$ equipped with the topology induced by the standard one of $\mathbb{R}^N$. Then we assume that
\begin{equation} \label{eq:param-prop}
    \begin{aligned}
         & \text{$\Xi$ is an open set}                                                       \\
         & \text{$\psi$ is a $\mathit{C}^\infty$ diffeomorphism from $\mathcal{S}$ to $\Xi$}
    \end{aligned}
\end{equation}
This allows us to differentiate the probability distributions with respect to the parameters so that $\partial_ip(x;\xi)$ is well defined, where we wrote $\partial_i\coloneqq\frac{\partial}{\partial\xi^{(i)}}$. These conditions also imply that the pair $\mathcal{S}$ and $\psi$ form a chart of $\mathcal{S}$. Then for any another statistical model on $\mathcal{S}$ with parametrization $\psi'\colon\mathcal{S}\to\Xi'\subseteq\mathbb{R}^n$ that follows \cref{eq:param-prop}, the composed function $\psi'\circ\psi^{-1}\colon\Xi\to\Xi'$ will be a $\mathit{C}^\infty$ diffeomorphism. By considering all the possible parametrizations of this kind we may treat $\mathcal{S}$ as a $\mathit{C}^\infty$-differentiable manifold, where statistical models are the charts and the different parametrizations are the coordinate systems; we call manifolds like these \emph{statistical manifolds}.

From our definitions, it is clear that the maximal dimension of a model is $n=N-1$ and that every statistical manifold is a submanifold of
\begin{equation}
    \mathcal{P}\coloneqq\{p\in\mathbb{R}^\mathcal{X}\mid p(x)>0 \quad \forall x\in\mathcal{X} \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1\}
\end{equation}
that we call the \emph{manifold of probability distributions}. Notice that $\mathcal{P}$ is the interior of the space of probability distributions, this is because from our definitions follows that every $(N-1)$-dimensional statistical manifold must be an open subset of $\mathcal{A}_1$.

\subsection{The tangent space and its representations} \label{ch:tgt-space}
We will now study tangent vectors of statistical manifolds looking for useful statistical interpretations of them. To do this we will use the fact that, as explained in \cref{ch:space-prob-dist}, $\mathcal{P}$ can be embedded in the space of random variables $\mathbb{R}^\mathcal{X}$. Then we can try to also embed the tangent spaces in $\mathbb{R}^\mathcal{X}$ in some meaningful ways, thus linking tangent vectors and random variables.

\subsubsection*{The mixture representation}
Since $\mathcal{P}$ is an open subset of the affine space $\mathcal{A}_1$ we can naturally identify the tangent space at every point with the displacement vector space
\begin{equation}\label{eq:disp-aff-space}
    \mathcal{A}_0\coloneqq\{A\in\mathbb{R}^\mathcal{X}\mid\sum_{x\in\mathcal{X}}A(x)=0\}
\end{equation}
This is the natural embedding of $T_p(\mathcal{P})$ that arises from the trivial embedding of $\mathcal{P}$ in $\mathbb{R}^\mathcal{X}$, in fact for any $X\in T_p(\mathcal{P})$ we can define
\begin{equation}\label{eq:mixt-rep}
    X^{(m)}(x)\coloneqq X(p(x))
\end{equation}
then by considering a parametrization $\{\xi^{(i)}\}$ of $\mathcal{P}$ and its relative coordinate basis $\{\partial_i\}$ we have that
\begin{equation}
    \partial_i^{(m)}(x)=\partial_ip(x;\xi)\in\mathcal{A}_0
\end{equation}
since
\begin{equation}\label{eq:mixt-zero-sum}
    \sum_{x\in\mathcal{X}}\partial_ip(x;\xi)=\partial_i\sum_{x\in\mathcal{X}}p(x;\xi)=0
\end{equation}
Finally, from \cref{eq:param-prop} follows that $\partial_ip(x;\xi)$ are $N-1$ linearly independent functions and thus
\begin{equation}
    X^{(m)}\leftrightarrow X
\end{equation}
is an isomorphism and
\begin{equation}
    T_p(\mathcal{P})\sim T_p^{(m)}(\mathcal{P})\coloneqq\{X^{(m)}\mid X\in T_p(\mathcal{P})\}=\mathcal{A}_0\qquad\forall p\in\mathcal{P}
\end{equation}
We call $X^{(m)}$ the \emph{mixture representation} or \emph{m-representation} of $X$.

\subsubsection*{The exponential representation}
Since $T_p(\mathcal{P})$ is an $(N-1)$-dimensional vector space, for every $p$ we may try to identify it to the subspace of $\mathbb{R}^\mathcal{X}$ orthogonal to $p$ with respect to the inner product defined in \cref{eq:inn-prod-Rx}. This is interesting given the statistical meaning of the inner product between a generic element of $\mathbb{R}^\mathcal{X}$ and a probability distribution. For every $p\in\mathcal{P}$ the orthogonal space is
\begin{equation}
    \mathcal{A}_{p}^{\bot}\coloneqq\{A\in\mathbb{R}^\mathcal{X}\mid p\cdot A=\mathrm{E}_p[A]=0\}
\end{equation}
that is the space of random variables with null expectation value when the underlying probability distribution is $p$.

Now we wish to find a natural isomorphism between $T_p(\mathcal{P})$ and $\mathcal{A}_{p}^{\bot}$. One way to do this that will prove to be useful is to consider the following alternative embedding of $\mathcal{P}$ in $\mathbb{R}^\mathcal{X}$
\begin{equation}
    p\mapsto \ln p\in\mathbb{R}^\mathcal{X}
\end{equation}
then, for any $X\in T_p(\mathcal{P})$ we can define
\begin{equation}
    X^{(e)}(x)\coloneqq X(\ln p(x))=\frac{X(p(x))}{p(x)}
\end{equation}
and by considering a parametrization $\{\xi^{(i)}\}$ of $\mathcal{P}$ and its relative coordinate basis $\{\partial_i\}$ we have that
\begin{equation}
    \partial_i^{(e)}(x)=\partial_i\ln p(x;\xi)\in\mathcal{A}_{p}^{\bot}
\end{equation}
since
\begin{equation}\label{eq:exp-zero-ev}
    \mathrm{E}_p[\partial_i\ln p(x;\xi)]=\sum_{x\in\mathcal{X}}p(x;\xi)\frac{\partial_ip(x;\xi)}{p(x;\xi)}=\sum_{x\in\mathcal{X}}\partial_ip(x;\xi)=0
\end{equation}
It's easy to prove that the linear independence of $\partial_i\ln p(x;\xi)$ follows from the one of $\partial_i p(x;\xi)$ and thus
\begin{equation}
    X^{(e)}\leftrightarrow X
\end{equation}
is an isomorphism and
\begin{equation}
    T_p(\mathcal{P})\sim T_p^{(e)}(\mathcal{P})\coloneqq\{X^{(e)}\mid X\in T_p(\mathcal{P})\}=\mathcal{A}_{p}^{\bot}\qquad\forall p\in\mathcal{P}
\end{equation}
We call $X^{(m)}$ the \emph{exponential representation} or \emph{e-representation} of $X$.

From their definitions, we have that the two representations of a tangent vector $X\in T_p(\mathcal{P})$ are related as follows
\begin{equation}\label{eq:rep-rel}
    X^{(m)}(x)=X^{(e)}(x)p(x)
\end{equation}
and that while $T_p^{(m)}(\mathcal{P})$ is the same for every $p$, $T_p^{(e)}(\mathcal{P})$ varies since the set of random variables with null expectation value will differ depending on the underlying probability distribution.