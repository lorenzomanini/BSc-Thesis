\section{Parameter estimation}\label{ch:param-est}

\subsection{Unbiased estimators}
Consider a random process and an $n$-dim statistical model $\mathcal{S}=\left\{ p_\xi \mid \xi\in\Xi \right\}$ of it, as defined in \cref{ch:stat-models}; it is often the case that from a measured sample $x\in\mathcal{X}$ we want to estimate the parameters $\xi$ of the underlying probability distribution, that we assume to be in $\mathcal{S}$.

The estimation is represented by a function
\begin{equation}
    \hat{\xi}=(\hat{\xi}^{(1)},\dots,\hat{\xi}^{(n)})\colon\mathcal{X}\to\Xi\subseteq\mathbb{R}^n
\end{equation}
that we call \emph{estimator}. Each component $\hat{\xi}^{(i)}$ is a random variable, and we say that $\hat{\xi}$ is an \emph{unbiased estimator} if
\begin{equation}
    \mathrm{E}_{p_\xi}\left[\,\hat{\xi}\,\right]= \left(\mathrm{E}_{p_\xi}\left[ \hat{\xi}^{(1)} \right],\dots,\mathrm{E}_{p_\xi}\left[ \hat{\xi}^{(n)} \right]\right)=\xi\qquad\forall \xi\in\Xi
\end{equation}
i.e. if for each $p_\xi\in\mathcal{S}$ the expectation value of the estimator is the correct parameter $\xi$.

Then for an unbiased estimator, we may represent the deviation from the true parameters with the variance-covariance matrix of the estimator $V_\xi\left[ \,\hat{\xi}\, \right]\coloneqq\left\{v_\xi^{ij}\right\}$ where
\begin{equation}
    v_\xi^{ij}\coloneqq \mathrm{Cov}_{p_\xi}\left[ \hat{\xi}^{(i)},\hat{\xi}^{(j)}\right]=\mathrm{E}_{p_\xi}\left[ \left( \hat{\xi}^{(i)}(x) - \xi^i \right) \left( \hat{\xi}^{(j)}(x) - \xi^j \right) \right]
\end{equation}
In particular, the elements on the diagonal are the variances of the components of the estimator.
\todo{Talk about the covariance ellipse?}

\subsection{Variance and expectation value}
For a generic random variable $A\in\mathbb{R}^\mathcal{X}$ we may define a real function $\mathrm{E}[A]$ on $\mathcal{P}$ that maps every probability distribution $p$ to the expectation value of $A$ when the sample is generated with $p$ as underlying probability distribution
\begin{equation}
    \mathrm{E}[A]\colon\mathcal{P}\to\mathbb{R}\qquad p\mapsto\mathrm{E}_p[A]
\end{equation}
Since this is a function of the manifold $\mathcal{P}$, at every point $p$ we may consider its differential $(d\mathrm{E}[A])_p$. This is the element of the cotangent space $T_p^*(\mathcal{P})$ such that for any tangent vector $X\in T_p(\mathcal{P})$ we have
\begin{equation}
    (d\mathrm{E}[A])_p(X)=X(\mathrm{E}[A])
\end{equation}
Also, because a metric is defined on $\mathcal{P}$ we have a natural isomorphism between tangent and cotangent vectors, thus the gradient of $\mathrm{E}[A]$ is the tangent vector defined by
\begin{equation} \label{eq:gradient}
    \langle (grad\mathrm{E}[A])_p, X \rangle_p = (d\mathrm{E}[A])_p(X)=X(\mathrm{E}[A])\qquad\forall X\in T_p(\mathcal{P})
\end{equation}

We now state and prove the following theorem that relates the deviation of a random variable to the gradient of its expectation value
\begin{theorem} \label{th:gradient}
    For any random variable $A\in\mathbb{R}^\mathcal{X}$ we have that
    \begin{equation}
        (grad\mathrm{E}[A])_p^{(e)}=A-\mathrm{E}_p[A] \qquad\forall p\in\mathcal{P}
    \end{equation}
    where the gradient is the dual tangent vector of the differential with respect to the Fisher metric.
\end{theorem}
\begin{proof}
    For every $X\in T_p$ we have
    \begin{align*}
        X(\mathrm{E}[A])&=\sum_{x\in\mathcal{X}}X(p(x))A(x)
        =\sum_{x\in\mathcal{X}}X^{(m)}(x)A(x)
        \\
        &=\mathrm{E}_p[X^{(e)}A]=\mathrm{E}_p[X^{(e)}(A-\mathrm{E}_p[A])]
    \end{align*}
    where in the last equation we used the fact that $\mathrm{E}_p[X^{(e)}]=0$. We notice that 
    \begin{equation*}
        \mathrm{E}_p[A-\mathrm{E}_p[A]]=0\implies (A-\mathrm{E}_p[A])\in T_p^{(e)}(\mathcal{P})
    \end{equation*}
    and so there must exist a tangent vector $Y\in T_p(\mathcal{P})$ such that $Y^{(e)}=(A-\mathrm{E}_p[A])$. Then
    \begin{equation*}
        X(\mathrm{E}[A])=\mathrm{E}_p[X^{(e)}Y^{(e)}]=\langle X,Y \rangle_p
    \end{equation*}
    and so from \cref{eq:gradient} we have that $Y=(grad\mathrm{E}[A])_p$ and so
    \begin{equation*}
        (grad\mathrm{E}[A])_p^{(e)}=A-\mathrm{E}_p[A]
    \end{equation*}
\end{proof}
We also get the following
\begin{corollary}\label{cr:variance}
    For any random variable $A$
    \begin{equation}
        \mathrm{V}_p[A]= \lVert (d\mathrm{E}[A])_p \rVert_p^2
    \end{equation}
\end{corollary}
\begin{proof}
    Follows from \cref{th:gradient} noticing that
    \begin{align*}
        \lVert (d\mathrm{E}[A])_p \rVert_p^2&=\langle (grad\mathrm{E}[A])_p,(grad\mathrm{E}[A])_p \rangle_p
        \\
        &=\mathrm{E}_p[(A-\mathrm{E}_p[A])^2]
    \end{align*}
\end{proof}

Let now $\mathcal{S}$ be an $n$-dim statistical manifold, since it is a submanifold of $\mathcal{P}$ we have that $T_p(\mathcal{S})$ is a linear subspace of $T_p(\mathcal{P})$ for every $p\in\mathcal{S}$. Then the gradient of the restriction on $\mathcal{S}$ of a function of $\mathcal{P}$ is its orthogonal projection on $T_p(\mathcal{S})$. In particular, we have that 
\begin{equation}
    T_p(\mathcal{P})=T_p(\mathcal{S})\oplus T_p(\mathcal{S})^\perp
\end{equation}
and so we may uniquely decompose
\begin{equation}
    (grad\mathrm{E}[A])_p=v_\parallel + v_\perp\qquad v_\parallel\in T_p(\mathcal{S}),v_\perp\in T_p(\mathcal{S})^\perp
\end{equation}
then we find that $\forall X\in T_p(\mathcal{S})$
\begin{equation} \label{eq:grad-projection}
    \langle (\left. grad\mathrm{E}[A] \right\rvert_\mathcal{S})_p, X \rangle_p = X(\mathrm{E}[A])= \langle v_\parallel + v_\perp, X\rangle_p = \langle v_\parallel, X\rangle_p
\end{equation}
and so $(\left. grad\mathrm{E}[A] \right\rvert_\mathcal{S})_p=v_\parallel$.

Finally, we have the following theorem relating the variance of a random variable and the sensitivity of its expectation value to the changes in the model parameters
\begin{theorem} \label{th:variance-ineq}
    Given a statistical manifold $S$, for any random variable $A$ we have that
    \begin{equation}
            \mathrm{V}_p[A]\ge \lVert (\left.d\mathrm{E}[A]\right\rvert_\mathcal{S})_p \rVert_p^2
    \end{equation}
    where the equality holds if and only if
    \begin{equation}
        A-\mathrm{E}_p[A]\in T_p^{(e)}(\mathcal{S})
    \end{equation}
\end{theorem}
\begin{proof}
    Follows immediately from \cref{cr:variance} and \cref{eq:grad-projection}
\end{proof}

\subsection{The Cramér-Rao bound}
We are now in the position to state and prove an important result of parameter estimation theory
\begin{theorem}[Cramér-Rao bound]\label{th:cramer-rao}
    Let $\mathcal{S}=\left\{ p_\xi \mid \xi\in\Xi \right\}$ be an $n$-dim statistical model of $\mathcal{P}$. Then, for any unbiased estimator $\hat{\xi}$ the variance-covariance matrix $\mathrm{V}_\xi[\hat{\xi}]$ satisfies
    \begin{equation}\label{eq:cramer-rao}
        \mathrm{V}_\xi[\hat{\xi}]\ge G_F^{-1}(p_\xi)
    \end{equation}
    in the sense that $\mathrm{V}_\xi[\hat{\xi}]-G_F^{-1}(p_\xi)$ is positive semi-definite.
\end{theorem}
\begin{proof}
    Let $A=c_i\hat{\xi}^{(i)}$ where $c$ is an arbitrary element of $\mathbb{R}^n$. Then $A$ is a random variable with $\mathrm{E}_{p_\xi}[A]=c_i\xi^i$ and
    \begin{align*}
        \mathrm{V}_{p_\xi}[A] &= \mathrm{E}_{p_\xi}[(c_i\hat{\xi}^{(i)}(x)-c_i\xi^i)(c_j\hat{\xi}^{(j)}(x)-c_j\xi^j)]
        \\
        &=\mathrm{E}_{p_\xi}[c_i(\hat{\xi}^{(i)}(x)-\xi^i)(\hat{\xi}^{(j)}(x)-\xi^j)c_j]
        \\
        &=c_i v_\xi^{ij} c_j
    \end{align*}
    Then letting $p=p_\xi$ from \cref{th:variance-ineq} we get
    \begin{equation*}
        c_i v_\xi^{ij} c_j\ge \lVert (\left.d\mathrm{E}[A]\right\rvert_\mathcal{S})_p \rVert_p^2
    \end{equation*}
    where, in the coordinate basis of $\{\xi^{(i)}\}$
    \begin{align*}
        \lVert (\left.d\mathrm{E}[A]\right\rvert_\mathcal{S})_p \rVert_p^2&=(\partial_i\mathrm{E}[A])_p g^{ij}(p)(\partial_j\mathrm{E}[A])_p
        \\
        &=c_i g^{ij}(p) c_j
    \end{align*}
    and so, finally
    \begin{equation*}
        c_i (v_\xi^{ij}-g^{ij}(p_\xi)) c_j \ge 0
    \end{equation*}
\end{proof}

An unbiased estimator that saturates \cref{eq:cramer-rao} is called an \emph{efficient estimator} and is the best unbiased estimator in the sense that its variance is minimum between all unbiased estimators. It's important to notice that an efficient estimator doesn't always exist. \todo{talk about asymptotically efficient estimators?}

This result shows that the efficiency with which we can infer the underlying probability distribution of a process is deeply linked with the information geometry of the model. \todo{Examples}