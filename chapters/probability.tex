\chapter{Geometry of probability distributions}
Quantum states, as we will see, can be thought of as a generalization of probability distributions. In this chapter, we will study probability distributions from a geometrical point of view, and in this framework, we shall prove the Cramer-Rao bound. We will only consider probability distributions defined on finite sample spaces since this is all we need for finite-dimensional pure quantum states; nevertheless, in \ref{} generalization to countable sample spaces will be discussed briefly.

\section{Statistical manifolds}
\subsection{Space of probability distributions}
Consider a \emph{random variable} that can take values in a finite set $\mathcal{X}$ of cardinality $N$ that we call \emph{sample space}. Then a \emph{probability distribution} on $\mathcal{X}$ is a function $p\colon\mathcal{X}\to\mathbb{R}$ which satisfies
\begin{equation}
    p(x)\ge 0 \quad \forall x\in\mathcal{X} \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1
\end{equation}
where $p(x)$ represents the probability that the random variable is found with value $x$. Accordingly, the \emph{space of probability distributions} is
\begin{equation}
    \mathcal{P} = \Bigl\{ p\colon\mathcal{X}\to\mathbb{R} \bigm\vert p(x)\ge 0 \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1 \Bigr\}
\end{equation}
that we consider equipped with the point-wise topology.
\todo{Decidere se parlare della topologia}

Since we are working with a finite sample space we can consider the isomorphism between functions $f\colon\mathcal{X}\to\mathbb{R}$ and $\mathbb{R}^{card(\mathcal{X})}=\mathbb{R}^N$ through 
\begin{equation*}
    f\leftrightarrow\boldsymbol{f}\coloneqq(f(x_1),f(x_2),\dots,f(x_N))
\end{equation*}
so that
\begin{equation}
    \mathcal{P}\sim\Bigl\{ \boldsymbol{p}\in\mathbb{R}^N \bigm\vert p_i\geq 0  \quad \text{and} \quad \sum p_i=1 \Bigr\}
\end{equation}
where the standard topology induced by this isomorphism corresponds to the point-wise one.
\todo{Decidere se parlare della topologia}

We can conclude that the space of probability distributions is the (N-1)-simplex generated by convex mixing of the trivial distributions, as seen in \ref{}; in \ref{} pictures for the first few dimensions are given.
\todo{Decidere se parlare degli spazi convessi}

\subsubsection{Examples}
\todo{Esempi}

\subsection{Statistical models}
We call an n-dimensional \emph{statistical model} on $\mathcal{X}$ a family of probability distributions that are globally parametrized by n real-valued variables. Formally this is a set $\mathcal{S}\subseteq\mathcal{P}$ with an invertible function $\psi\colon\mathcal{S}\to\Xi\subseteq\mathbb{R}^n$, so that we may write
\begin{equation}
    \mathcal{S} = \Bigl\{ p_\xi\in\mathcal{P} \bigm\vert \exists\xi=[\xi^1,\xi^2,\dots,\xi^n]\in\Xi \colon p_\xi=\psi^{-1}(\xi) \Bigr\}
\end{equation}
where $p_\xi(x)$ may be equivalently written as $p(x;\xi)$ or $p(x;\xi^1,\xi^2,\dots,\xi^n)$. This definition of a statistical model reflects the act of hypothesizing an underlying model, that may depend on some parameters, for the generation of the random variable's samples. Then only a subset, here represented by $\mathcal{S}$, of all the possible probability distributions is considered as a candidate of the underlying probability distribution, and every candidate probability distribution is identified uniquely by the corresponding parameters, here represented by $\xi$.

We also require two important additional regularity properties:
\begin{equation}
    \text{$\mathcal{S}$ is an open set}\qquad\text{and}\qquad\text{$\psi^{-1}$ is $\mathit{C}^\infty$}
\end{equation}
that immediately imply that $\Xi$ is also an open set. This allows us to differentiate the probability distributions with respect to the parameters so that $\partial_ip(x;\xi)$ is well defined, where we wrote $\partial_i\coloneqq\frac{\partial}{\partial\xi_i}$. These conditions also imply that the pair $\mathcal{S}$ and $\psi$ form a chart (both of $\mathcal{P}$ and $\mathcal{S}$). Then by taking parametrizations that are $\mathit{C}^\infty$ diffeomorphic to each other, we can construct $\mathit{C}^\infty$ manifolds as described in \ref{}, where statistical models are the charts and the different parametrizations are the coordinate systems; we call manifolds like these \emph{statistical manifolds}.

\subsubsection{Examples}
\todo{Esempi}

\section{The Fisher information metric}
\subsection{The information divergence}
Given a statistical manifold, we may consider its tangent bundle and ask ourselves if a certain metric can be naturally defined on it. Such a metric would give rise to a Riemaniann connection and consequently to a geodesic distance between elements of the manifold. For this reason, we should first find a statistical meaning to the notion of distance between probability distributions.

One natural way to proceed is to consider how hard it is to distinguish a probability distribution from another one by extracting some samples. More precisely let's assume that a random variable has an underlying distribution $q$ and that $N_s$ samples are extracted. Then we can consider the probability that the resulting frequencies $f_i$ of the samples correspond to the probabilities $p_i$ of another probability distribution $p$.

For simplicity, let's consider the $N=2$ case, i.e. the case of binomial distributions. Let $\mathbf{q}=(t,1-t)$ and $\mathbf{p}=(r,1-r)$ be two probability distributions. Then if $N_s$ samples are drawn with underlying probability $q$, the probability $P_{N_s}(\mathbf{p})$ that the obtained frequencies correspond to $\mathbf{p}$ is given by
\begin{equation*}
    P_{N_s}(\mathbf{p})=\binom{N_s}{N_s\cdot r}t^{\,N_s\cdot r}(1-t)^{\,N_s\cdot (1-r)}
\end{equation*}
then by assuming $r\ne 0,1$ and using Stirling's formula, we obtain the following asymptotic behavior for $N_s\to\infty$
\begin{equation}
    P_{N_s}(\mathbf{p})\sim\exp\biggl\{-N_s\biggl[r\ln\biggl(\frac{r}{t}\biggr)+(1-r)\ln\biggl(\frac{1-r}{1-t}\biggr)\biggr]\biggr\}
\end{equation}
so the probability decreases exponentially with $N_s$ times the factor in the square parenthesis. This factor only depends on the probability distributions $p$ and $q$, and one may recognize it from statistics as the \emph{relative entropy} of the two distributions. For a generic finite sample space $\mathcal{X}$, the relative entropy is defined as
\begin{equation} \label{eq:rel-entropy}
    S(p\parallel q)\coloneqq\sum_{x\in\mathcal{X}}p(x)\ln\biggl(\frac{p(x)}{q(x)}\biggr)
\end{equation}

More generally it can be shown that the following theorem holds.
\begin{theorem}[Sanov's Theorem]
    Let $\mathcal{P}$ be the set of probability distributions on a finite sample space $\mathcal{X}$ and $\mathcal{E}\subseteq\mathcal{P}$ be a closed set without isolated points. Then if $N_s$ samples are drawn with underlying probability distribution $q\in\mathcal{P}$, the probability $P_{N_s}(\mathcal{E})$ that the obtained frequencies correspond to an element in $\mathcal{E}$ has the following asymptotic behavior
    \begin{equation}
        P_{N_s}(\mathcal{E})\sim e^{-N_sS(p_*\parallel q)}\qquad\text{for}\quad N_s\to\infty
    \end{equation}
    where $p_*$ is the element of $\mathcal{E}$ for which $S(p_*\parallel q)$ is smallest.
\end{theorem}
Roughly speaking, this shows that the greater the relative entropy $S(p\parallel q)$ the faster the probability of obtaining frequencies in a small neighborhood of $p$ decreases with the number of samples drawn with underlying probability $q$. In this view, relative entropy can serve as a kind of distance between probability distributions, but with some caveats. From the definition in \cref{eq:rel-entropy} relative entropy has the following properties
\begin{gather}
    S(p\parallel q)\ge 0\quad\forall p,q\in\mathcal{P}\\
    S(p\parallel q)=0\iff p=q
\end{gather}
but it is not symmetric and it doesn't follow the triangle inequality, so it is not a metric distance. We may ask ourselves if the asymmetry is an accident of our definition of relative entropy or if it is inherent in the distinguishability of probability distributions. The latter turns out to be true, as shown by the following example.
\begin{example}
    Consider two coins, one fair and one with heads on both sides. We want to pick one and guess which one it is just by tossing it multiple times. Clearly, the game is not symmetric in the choice of the coin; in fact, if we pick the fair coin the first time we will get a tail we will be sure that we picked the fair one, while if we pick the double-head one we will only get heads but this result will always be also compatible with a fair coin that, by chance, is only giving heads.
\end{example}
This game is precisely a problem of distinguishability of probability distributions. In fact, we have the $N=2$ sample space and two probability distributions: $\mathbf{p}=(0.5,0.5)$ (the fair coin) and $\mathbf{q}=(1,0)$ (the double-head coin), so the two relative entropies are $S(p\parallel q)=\infty$ and $S(q\parallel p)=\ln 2$. If we pick the fair coin for large $N_s$ the obtained frequencies will approach $\mathbf{p}$; then we consider the probability of obtaining these frequencies if the underlying distribution was $q$. This probability is identically 0, and it is coherent with Sanov's theorem since  $S(p\parallel q)=\infty$ and so $P_{N_s}(p)\sim e^{-N_s\cdot\infty}=0$. Otherwise, if we pick the double-head coin the frequencies will always match exactly $\mathbf{q}$, and the probability of getting this if the underlying distribution was $q$ is $0.5^{N_s}$. This is coherent with Sanov's theorem since  $S(q\parallel p)=\ln 2$ and so $P_{N_s}(q)\sim e^{-N_s\cdot\ln 2}=0.5^{N_s}$