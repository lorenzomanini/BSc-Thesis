\chapter{Geometry of probability distributions}
Quantum states, as we will see, can be thought of as a generalization of probability distributions. In this chapter, we will study probability distributions from a geometrical point of view, and in this framework, we shall prove the Cramer-Rao bound. We will only consider probability distributions defined on finite sample spaces since this is all we need for finite-dimensional pure quantum states; nevertheless, in \ref{} generalization to countable sample spaces will be discussed briefly.

\section{Statistical manifolds}
\subsection{Space of probability distributions}
Consider a \emph{random variable} that can take values in a finite set $\mathcal{X}$ of cardinality $N$ that we call \emph{sample space}. Then a \emph{probability distribution} on $\mathcal{X}$ is a function $p\colon\mathcal{X}\to\mathbb{R}$ which satisfies
\begin{equation}
    p(x)\ge 0 \quad \forall x\in\mathcal{X} \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1
\end{equation}
where $p(x)$ represents the probability that the random variable is found with value $x$. Accordingly, the \emph{space of probability distributions} is
\begin{equation}
    \mathcal{P} = \Bigl\{ p\colon\mathcal{X}\to\mathbb{R} \bigm\vert p(x)\ge 0 \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1 \Bigr\}
\end{equation}
that we consider equipped with the point-wise topology.
\todo{Decidere se parlare della topologia}

Since we are working with a finite sample space we can consider the isomorphism between functions $f\colon\mathcal{X}\to\mathbb{R}$ and $\mathbb{R}^{card(\mathcal{X})}=\mathbb{R}^N$ through 
\begin{equation*}
    f\leftrightarrow\boldsymbol{f}\coloneqq(f(x_1),f(x_2),\dots,f(x_N))
\end{equation*}
so that
\begin{equation}
    \mathcal{P}\sim\Bigl\{ \boldsymbol{p}\in\mathbb{R}^N \bigm\vert p_i\geq 0  \quad \text{and} \quad \sum p_i=1 \Bigr\}
\end{equation}
where the standard topology induced by this isomorphism corresponds to the point-wise one.
\todo{Decidere se parlare della topologia}

We can conclude that the space of probability distributions is the (N-1)-simplex generated by convex mixing of the trivial distributions, as seen in \ref{}; in \ref{} pictures for the first few dimensions are given.
\todo{Decidere se parlare degli spazi convessi}

\subsubsection{Examples}
\todo{Esempi}

\subsection{Statistical models}
We call an n-dimensional \emph{statistical model} on $\mathcal{X}$ a family of probability distributions that are globally parametrized by n real-valued variables. Formally this is a set $\mathcal{S}\subseteq\mathcal{P}$ with an invertible function $\psi\colon\mathcal{S}\to\Xi\subseteq\mathbb{R}^n$, so that we may write
\begin{equation}
    \mathcal{S} = \Bigl\{ p_\xi\in\mathcal{P} \bigm\vert \exists\xi=[\xi^1,\xi^2,\dots,\xi^n]\in\Xi \colon p_\xi=\psi^{-1}(\xi) \Bigr\}
\end{equation}
where $p_\xi(x)$ may be equivalently written as $p(x;\xi)$ or $p(x;\xi^1,\xi^2,\dots,\xi^n)$. This definition of a statistical model reflects the act of hypothesizing an underlying model, that may depend on some parameters, for the generation of the random variable's samples. Then only a subset, here represented by $\mathcal{S}$, of all the possible probability distributions is considered as a candidate of the underlying probability distribution, and every candidate probability distribution is identified uniquely by the corresponding parameters, here represented by $\xi$.

We also require two important additional regularity properties:
\begin{equation}
    \text{$\mathcal{S}$ is an open set}\qquad\text{and}\qquad\text{$\psi^{-1}$ is $\mathit{C}^\infty$}
\end{equation}
that immediately imply that $\Xi$ is also an open set. This allows us to differentiate the probability distributions with respect to the parameters so that $\partial_ip(x;\xi)$ is well defined, where we wrote $\partial_i\coloneqq\frac{\partial}{\partial\xi_i}$. These conditions also imply that the pair $\mathcal{S}$ and $\psi$ form a chart (both of $\mathcal{P}$ and $\mathcal{S}$). Then by taking parametrizations that are $\mathit{C}^\infty$ diffeomorphic to each other, we can construct $\mathit{C}^\infty$ manifolds as described in \ref{}, where statistical models are the charts and the different parametrizations are the coordinate systems; we call manifolds like these \emph{statistical manifolds}.

\subsubsection{Examples}
\todo{Esempi}

\section{The Fisher information metric}
\subsection{The information divergence}
Given a statistical manifold, we may consider its tangent bundle and ask ourselves if a certain metric can be naturally defined on it. Such a metric would give rise to a Riemaniann connection and consequently to a geodesic distance between elements of the manifold. For this reason, we should first find a statistical meaning to the notion of distance between probability distributions.

One natural way to proceed is to consider how hard it is to distinguish a probability distribution from another one by extracting some samples. More precisely let's assume that a random variable has an underlying distribution $q$ and that $N_s$ samples are extracted. Then we can consider the probability that the resulting frequencies $f_i$ of the samples correspond to the probabilities $p_i$ of another probability distribution $p$.

For simplicity, let's consider the $N=2$ case, i.e. the case of binomial distributions. Let $\mathbf{q}=(t,1-t)$ and $\mathbf{p}=(r,1-r)$ be two probability distributions. Then if $N_s$ samples are drawn with underlying probability $q$, the probability $P_{N_s}(\mathbf{p})$ that the obtained frequencies correspond to $\mathbf{p}$ is given by
\begin{equation*}
    P_{N_s}(\mathbf{p})=\binom{N_s}{N_s\cdot r}t^{N_s\cdot r}(1-t)^{N_s\cdot (1-r)}
\end{equation*}
then by assuming $r\ne 0,1$ and using Stirling's formula, we obtain the following asymptotic behavior for $N_s\to\infty$
\begin{equation}
    P_{N_s}(\mathbf{p})\sim\exp\biggl\{-N_s\biggl[r\ln\biggl(\frac{r}{t}\biggr)+(1-r)\ln\biggl(\frac{1-r}{1-t}\biggr)\biggr]\biggr\}
\end{equation}
so the probability decreases exponentially with $N_s$ times the factor in the square parenthesis. This factor only depends on the probability distributions $p$ and $q$, and one may recognize it from statistics as the \emph{relative entropy} of the two distributions. For a generic finite sample space $\mathcal{X}$, the relative entropy is defined as
\begin{equation}
    S(p\parallel q)\coloneqq\sum_{x\in\mathcal{X}}p(x)\ln\biggl(\frac{p(x)}{q(x)}\biggr)
\end{equation}
More generally it can be shown that the following theorem holds.
\newpage
\begin{theorem}[Sanov's Theorem]
    Let $\mathcal{P}$ be the set of probability distributions on a finite sample space $\mathcal{X}$ and $\mathcal{E}\subseteq\mathcal{P}$ be a closed set without isolated points. Then if $N_s$ samples are drawn with underlying probability distribution $q\in\mathcal{P}$, the probability $P_{N_s}(\mathcal{E})$ that the obtained frequencies correspond to an element in $\mathcal{E}$ has the following asymptotic behavior
    \begin{equation}
        P_{N_s}(\mathcal{E})\sim e^{-N_sS(p_*\parallel q)}\qquad\text{for}\quad N_s\to\infty
    \end{equation}
    where $p_*$ is the element of $\mathcal{E}$ for which $S(p_*\parallel q)$ is smallest.
\end{theorem}
Roughly speaking, this shows that the greater the relative entropy $S(p_*\parallel q)$ the faster the probability of obtaining frequencies in a small neighborhood of $p$ decreases with the number of samples drawn.