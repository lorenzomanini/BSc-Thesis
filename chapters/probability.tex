\chapter{Geometry of probability distributions}
Quantum states, as we will see, can be thought of as a generalization of probability distributions. In this chapter, we will study probability distributions from a geometrical point of view, and in this framework, we shall prove the Cramer-Rao bound. We will only consider probability distributions defined on finite sample spaces since this is all we need for finite-dimensional pure quantum states; nevertheless, in \ref{} generalization to countable sample spaces will be discussed briefly.

\section{Statistical manifolds}
\subsection{Space of probability distributions}
Consider a \emph{random variable} that can take values in a finite set $\mathcal{X}$ of cardinality $N$ that we call \emph{sample space}. Then a \emph{probability distribution} on $\mathcal{X}$ is a function $p\colon\mathcal{X}\to\mathbb{R}$ which satisfies
\begin{equation}
    p(x)\ge 0 \quad \forall x\in\mathcal{X} \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1
\end{equation}
where $p(x)$ represents the probability that the random variable is found with value $x$. Accordingly, the \emph{space of probability distributions} is
\begin{equation}
    \mathcal{P} = \Bigl\{ p\colon\mathcal{X}\to\mathbb{R} \bigm\vert p(x)\ge 0 \quad \text{and} \quad \sum_{x\in\mathcal{X}}p(x)=1 \Bigr\}
\end{equation}
that we consider equipped with the point-wise topology.
\todo{Decidere se parlare della topologia}

Since we are working with a finite sample space we can consider the isomorphism between functions $f\colon\mathcal{X}\to\mathbb{R}$ and $\mathbb{R}^{card(\mathcal{X})}=\mathbb{R}^N$ through 
\begin{equation*}
    f\leftrightarrow\boldsymbol{f}\coloneqq(f(x_1),f(x_2),\dots,f(x_N))
\end{equation*}
so that
\begin{equation}
    \mathcal{P}\sim\Bigl\{ \boldsymbol{p}\in\mathbb{R}^N \bigm\vert p_i\geq 0  \quad \text{and} \quad \sum p_i=1 \Bigr\}
\end{equation}
where the standard topology induced by this isomorphism corresponds to the point-wise one.
\todo{Decidere se parlare della topologia}

We can conclude that the space of probability distributions is the (N-1)-simplex generated by convex mixing of the trivial distributions, as seen in \ref{}; in \ref{} pictures for the first few dimensions are given.
\todo{Decidere se parlare degli spazi convessi}

\subsubsection{Examples}
\todo{Esempi}

\subsection{Statistical models}
We call an n-dimensional \emph{statistical model} on $\mathcal{X}$ a family of probability distributions that are globally parametrized by n real-valued variables. Formally this is a set $\mathcal{S}\subseteq\mathcal{P}$ with an invertible function $\psi\colon\mathcal{S}\to\Xi\subseteq\mathbb{R}^n$, so that we may write
\begin{equation}
    \mathcal{S} = \Bigl\{ p_\xi\in\mathcal{P} \bigm\vert \exists\xi=[\xi^1,\xi^2,\dots,\xi^n]\in\Xi \colon p_\xi=\psi^{-1}(\xi) \Bigr\}
\end{equation}
where $p_\xi(x)$ may be equivalently written as $p(x;\xi)$ or $p(x;\xi^1,\xi^2,\dots,\xi^n)$. This definition of a statistical model reflects the act of hypothesizing an underlying model, that may depend on some parameters, for the generation of the random variable's samples. Then only a subset, here represented by $\mathcal{S}$, of all the possible probability distributions is considered as a candidate of the underlying probability distribution, and every candidate probability distribution is identified uniquely by the corresponding parameters, here represented by $\xi$.

We also require two important additional regularity properties:
\begin{equation}
    \text{$\mathcal{S}$ is an open set}\qquad\text{and}\qquad\text{$\psi^{-1}$ is $\mathit{C}^\infty$}
\end{equation}
that immediately imply that $\Xi$ is also an open set. This allows us to differentiate the probability distributions with respect to the parameters so that $\partial_ip(x;\xi)$ is well defined, where we wrote $\partial_i\coloneqq\frac{\partial}{\partial\xi_i}$. These conditions also imply that the pair $\mathcal{S}$ and $\psi$ form a chart (both of $\mathcal{P}$ and $\mathcal{S}$). Then by taking parametrizations that are $\mathit{C}^\infty$ diffeomorphic to each other, we can construct $\mathit{C}^\infty$ manifolds as described in \ref{}, where statistical models are the charts and the different parametrizations are the coordinate systems; we call manifolds like these \emph{statistical manifolds}.

\subsubsection{Examples}
\todo{Esempi}

\section{Fisher information metric}
\subsection{Relative entropy} \label{ch:rel-entr}
Given a statistical manifold, we may consider its tangent bundle and ask ourselves if a certain metric can be naturally defined on it. Such a metric would give rise to a Riemannian connection and consequently to a geodesic distance between elements of the manifold. For this reason, we should first find a statistical meaning to the notion of distance between probability distributions.

One natural way to proceed is to consider how hard it is to distinguish a probability distribution from another one by extracting some samples. More precisely let's assume that a random variable has an underlying distribution $q$ and that $N_s$ samples are extracted. Then we can consider the probability that the resulting frequencies $f_i$ of the samples correspond to the probabilities $p_i$ of another probability distribution $p$.

For simplicity, let's consider the $N=2$ case, i.e. the case of binomial distributions. Let $\mathbf{q}=(t,1-t)$ and $\mathbf{p}=(r,1-r)$ be two probability distributions. Then if $N_s$ samples are drawn with underlying probability $q$, the probability $P_{N_s}(\mathbf{p})$ that the obtained frequencies correspond to $\mathbf{p}$ is given by
\begin{equation*}
    P_{N_s}(\mathbf{p})=\binom{N_s}{N_s\cdot r}t^{\,N_s\cdot r}(1-t)^{\,N_s\cdot (1-r)}
\end{equation*}
then by assuming $r\ne 0,1$ and using Stirling's formula, we obtain the following asymptotic behavior for $N_s\to\infty$
\begin{equation}
    P_{N_s}(\mathbf{p})\sim\exp\biggl\{-N_s\biggl[r\ln\biggl(\frac{r}{t}\biggr)+(1-r)\ln\biggl(\frac{1-r}{1-t}\biggr)\biggr]\biggr\}
\end{equation}
so the probability decreases exponentially with $N_s$ times the factor in the square parenthesis. This factor only depends on the probability distributions $p$ and $q$, and one may recognize it from statistics as the \emph{relative entropy} of the two distributions. For a generic finite sample space $\mathcal{X}$, the relative entropy is defined as
\begin{equation} \label{eq:rel-entropy}
    S(p\parallel q)\coloneqq\sum_{x\in\mathcal{X}}p(x)\ln\biggl(\frac{p(x)}{q(x)}\biggr)
\end{equation}

More generally it can be shown that the following theorem holds.
\begin{theorem}[Sanov's Theorem]
    Let $\mathcal{P}$ be the set of probability distributions on a finite sample space $\mathcal{X}$ and $\mathcal{E}\subseteq\mathcal{P}$ be a closed set without isolated points. Then if $N_s$ samples are drawn with underlying probability distribution $q\in\mathcal{P}$, the probability $P_{N_s}(\mathcal{E})$ that the obtained frequencies correspond to an element in $\mathcal{E}$ has the following asymptotic behavior
    \begin{equation}
        P_{N_s}(\mathcal{E})\sim e^{-N_sS(p_*\parallel q)}\qquad\text{for}\quad N_s\to\infty
    \end{equation}
    where $p_*$ is the element of $\mathcal{E}$ for which $S(p_*\parallel q)$ is smallest.
\end{theorem}
Roughly speaking, this shows that the greater the relative entropy $S(p\parallel q)$ the faster the probability of obtaining frequencies in a small neighborhood of $p$ decreases with the number of samples drawn with underlying probability $q$. In this view, relative entropy can serve as a kind of distance between probability distributions, but with some caveats. From the definition in \cref{eq:rel-entropy} relative entropy has the following properties
\begin{gather}
    S(p\parallel q)\ge 0\quad\forall p,q\in\mathcal{P}\\
    S(p\parallel q)=0\iff p=q
\end{gather}
but it is not symmetric and it doesn't follow the triangle inequality, so it is not a metric distance. We may ask ourselves if the asymmetry is an accident of our definition of relative entropy or if it is inherent in the distinguishability of probability distributions. The latter turns out to be true, as shown by the following example.
\begin{example}
    Consider two coins, one fair and one with heads on both sides. We want to pick one and guess which one it is just by tossing it multiple times. Clearly, the game is not symmetric in the choice of the coin; in fact, if we pick the fair coin the first time we will get a tail we will be sure that we picked the fair one, while if we pick the double-head one we will only get heads but this result will always be also compatible with a fair coin that, by chance, is only giving heads.
\end{example}
This game is precisely a problem of distinguishability of probability distributions. In fact, we have the $N=2$ sample space and two probability distributions: $\mathbf{p}=(0.5,0.5)$ (the fair coin) and $\mathbf{q}=(1,0)$ (the double-head coin), so the two relative entropies are $S(p\parallel q)=\infty$ and $S(q\parallel p)=\ln 2$. If we pick the fair coin for large $N_s$ the obtained frequencies will approach $\mathbf{p}$; then we consider the probability of obtaining these frequencies if the underlying distribution was $q$. This probability is identically 0, and it is coherent with Sanov's theorem since  $S(p\parallel q)=\infty$ and so $P_{N_s}(p)\sim e^{-N_s\cdot\infty}=0$. Otherwise, if we pick the double-head coin the frequencies will always match exactly $\mathbf{q}$, and the probability of getting this if the underlying distribution was $q$ is $0.5^{N_s}$. This is coherent with Sanov's theorem since  $S(q\parallel p)=\ln 2$ and so $P_{N_s}(q)\sim e^{-N_s\cdot\ln 2}=0.5^{N_s}$.

Even though relative entropy doesn't follow triangle inequality, it does follow a generalization of the Pythagorean theorem stated as follows
\begin{theorem}[Generalized Pythagorean theorem]
    Let $\mathcal{E}\subset\mathcal{P}$ be a convex set and consider $p\in\mathcal{E}$ and $q\in\mathcal{P}\setminus\mathcal{E}$. Then
    \begin{equation} \label{eq:gen-pyth}
        S(p\parallel q)\le S(p\parallel p_*)+S(p_*\parallel q)
    \end{equation}
    where $p_*=\min\limits_{r\in\partial\mathcal{E}}S(r\parallel q)$.
\end{theorem}
This is a generalization of the Pythagorean theorem in the sense that if it was stated in terms of the Euclidean distance squared, the angle between $\overline{pp_*}$ and $\overline{p_*q}$ would be convex and so \cref{eq:gen-pyth} would be the corollary of the Pythagorean theorem for convex triangles. This suggests that the relative entropy may be regarded as an asymmetric distance squared and as we will see this is enough to define a metric on the manifold.

\subsection{Squared Riemannian distances} \label{ch:square-dist} \todo{Find better title: maybe Metric Riemannian manifolds?}
Now that we have some notion of distance, we shall explore how to define a coherent metric on the manifold. First, we shall study this for a Riemannian distance as defined in \ref{}.

Let $(M,g)$ be a Riemannian manifold where $g$ is the metric tensor, then consider a point $p\in M$. We define the \emph{exponential map} in $p$ as follows
\begin{equation}
    \text{Exp}_p:T_pM\to M,\quad \text{Exp}_p(\mathbf{v})\coloneqq\gamma_{\mathbf{v}}(1)
\end{equation}
where $\mathbf{v}\in T_p$ and $\gamma_{\mathbf{v}}:[0,1]\to M$ is the unique geodesic tangent to $v$ in $p$, i.e. satisfying $\gamma_{\mathbf{v}}(0)=p$ and $\gamma_{\mathbf{v}}'(0)=\mathbf{v}$. In general, this map will be well-defined only from a neighborhood of the origin of $T_p$ to a neighborhood of $p$, since only locally the uniqueness of the geodesic curve is guaranteed. By eventually further restricting the neighborhood, this map will also be 1-1 since locally the geodesic curves don't cross.

Then in this neighborhood, we have the inverse of the exponential map that maps $q\mapsto\mathbf{v}_q\in T_pM$ so that $\gamma_{\mathbf{v}_q}(1)=q$. Since $\gamma_{\mathbf{v}_q}$ is the only geodesic connecting $p$ and $q$, the geodesic distance between them will be
\begin{equation}
    L=\int_{0}^{1}\sqrt{g_{\gamma_{\mathbf{v}_q}(\lambda)}(\gamma_{\mathbf{v}_q}'(\lambda),\gamma_{\mathbf{v}_q}'(\lambda))}d\lambda
\end{equation}
Because $\gamma_{\mathbf{v}_q}$ is a geodesic, by definition we have that $\gamma_{\mathbf{v}_q}'(\lambda)$ is parallel transported along the curve and so
\begin{equation}
    g_{\gamma_{\mathbf{v}_q}(\lambda)}(\gamma_{\mathbf{v}_q}'(\lambda),\gamma_{\mathbf{v}_q}'(\lambda))=g_p(\mathbf{v}_q,\mathbf{v}_q)\quad\forall\lambda\in [0,1]
\end{equation}
Finally then, we get
\begin{equation} \label{eq:geo-dist}
    L(p,q)=\int_{0}^{1}\sqrt{g_p(\mathbf{v}_q,\mathbf{v}_q)}d\lambda=\sqrt{g_p(\mathbf{v}_q,\mathbf{v}_q)}=\lVert \mathbf{v}_q \rVert
\end{equation}
and so $\mathbf{\hat{v}}_q={\mathbf{v}_q}\backslash{L(p,q)}$.

Now chose a vector $\mathbf{dp}\in T_pM$ and let $q=\text{Exp}_p(\mathbf{dp})$. Then, let $\Gamma_{\hat{\mathbf{dp}}}$ be the unique geodesic curve such that $\Gamma_{\hat{\mathbf{dp}}}(0)=p$ and $\Gamma_{\hat{\mathbf{dp}}}'(0)=\mathbf{\hat{dp}}$. Clearly, this is the following reparametrization of $\gamma_{\mathbf{dp}}$
\begin{equation}
    \Gamma_{\hat{\mathbf{dp}}}(l)=\gamma_{\mathbf{dp}}\biggl(\frac{l}{\lVert \mathbf{v}_q \rVert}\biggr)=\gamma_{\mathbf{dp}}\biggl(\frac{l}{L(p,q)}\biggr)
\end{equation}
and so $q=\Gamma_{\hat{\mathbf{dp}}}(L(p,q))$.

Now let $\{x^{(i)}\}$ be a coordinate system for the neighborhood; from the Taylor expansion of $\Gamma_{\hat{\mathbf{dp}}}^{(i)}(l)$ in $l=0$ we get
\begin{equation}
    \Gamma_{\hat{\mathbf{dp}}}^{(i)}(l)=\Gamma_{\hat{\mathbf{dp}}}^{(i)}(0)+\left.\frac{d\Gamma_{\hat{\mathbf{dp}}}^{(i)}(l)}{dl}\right\rvert_{l=0}\cdot l+O(l^2)
\end{equation}
then, from $\Gamma_{\hat{\mathbf{dp}}}(0)=p$, $\Gamma_{\hat{\mathbf{dp}}}'(0)=\mathbf{\hat{dp}}$ and $q=\Gamma_{\hat{\mathbf{dp}}}(L(p,q))$ we get that
\begin{equation} \label{eq:p-dp}
    q^{(i)}=p^{(i)}+\hat{dp^i}\,L(p,q)+O(L^2(p,q))=p^{(i)}+dp^i+O(\lVert \mathbf{dp} \rVert^2)
\end{equation}
for every $q$ in the image of $\Gamma_{\hat{\mathbf{dp}}}$. Then, combining \cref{eq:geo-dist} and \cref{eq:p-dp} we get
\begin{equation}
    L^2(\{p^{(i)}\},\{p^{(i)}+dp^i\})\to L^2(p,q)=g_{ij}dp^i dp^j \quad \text{for} \quad dp^i\to 0
\end{equation}

We define $L_p^2:M\to [0,+\infty)$, $L_p^2(q)\coloneqq L^2(p,q)$ and consider its Taylor expansion in $q=p$
\begin{equation}
    L_p^2(\{p^{(i)}+dp^i\})=\frac{1}{2}\Bigl[\partial_i\partial_j L_p^2(q)\Bigr]_{q=p} dp^i dp^j + O(dp^3)
\end{equation}
where the first derivative vanishes because $p$ is a minimum of $L_p^2$. Then we get
\begin{equation}
    g_{ij}^{(p)}=\frac{1}{2}\Bigl[\partial_i\partial_j L_p^2(q)\Bigr]_{q=p}
\end{equation}
so the metric tensor in $p$ is proportional to the Hessian matrix of $L_p^2$ in $p$. This result allows us to recover the metric tensor from the squared Riemannian distance.

\subsection{Divergences}
As argued in the last paragraph of \cref{ch:rel-entr}, relative entropy has some properties of squared distances and so we may try to find a metric tensor coherent with it as we did in \cref{ch:square-dist}.

Let $M$ be an analytic manifold and $D(\,\cdot\parallel\cdot\,):M\times M\to[o,+\infty)$ a $C^2$ function (possibly asymmetric) satisfying
\begin{equation} \label{eq:div-def}
    D(p\parallel q)\ge 0\quad\text{and}\quad D(p\parallel q)=0\iff p=q\qquad \forall p,q\in M
\end{equation}
Then, given a coordinate system $\{\xi^{(i)}\}$ on $M$ we have that every pair of points $(q,q')\in M\times M$ has coordinates $(\{\xi^{(i)}\},\{\xi^{(i)'}\})$ and we use the following notation for partial derivatives in one of the two terms on the diagonal $(p,p)$
\begin{gather*}
    D[\,\partial_i\parallel\cdot\,]\colon p\mapsto \left[\partial_i D(q\parallel q')\right]_{(q,q')=(p,p)}\\
    D[\,\cdot\parallel\partial_i\,]\colon p\mapsto \left[\partial_i' D(q\parallel q')\right]_{(q,q')=(p,p)}
\end{gather*}

From the fact that the diagonal $(p,p)$ is a constant surface of minima of $D$ follows that
\begin{equation} \label{eq:div-firstd}
    D[\,\partial_i\parallel\cdot\,]=D[\,\cdot\parallel\partial_i\,]\equiv 0
\end{equation}
so the diagonal is also a constant surface of the derivatives of $D$. Then by further deriving parallel to the diagonal, we get
\begin{gather*}
    (\partial_i+\partial_i') D[\,\cdot\parallel\partial_j\,]=D[\,\cdot\parallel\partial_i\partial_j\,]+D[\,\partial_i\parallel\partial_j\,]\equiv 0\\
    (\partial_j+\partial_j')D[\,\partial_i\parallel\cdot\,]=D[\,\partial_i\partial_j\parallel\cdot\,]+D[\,\partial_i\parallel\partial_j\,]\equiv 0
\end{gather*}
where we used the fact that since $D$ is $C^2$ we can swap second-order derivatives. Finally, we get
\begin{equation} \label{eq:div-secd}
    D[\,\partial_i\partial_j\parallel\cdot\,]=D[\,\cdot\parallel\partial_i\partial_j\,]=-D[\,\partial_i\parallel\partial_j\,]\eqqcolon g_{ij}^{(D)}
\end{equation}
where from the fact that the diagonal is a surface of minima, it follows that the previous expression defines a (symmetric) positive semi-definite matrix. It can further be shown that the Hessian matrix of any $C^2$ function of the manifold evaluated in a critical point is a $\left(\frac{0}{2}\right)$ tensor. From \cref{eq:div-firstd} and \cref{eq:div-secd} it follows that denoting
\begin{equation*}
    D_p^{(R)}: q\mapsto D(p\parallel q) \quad\text{and}\quad D_p^{(L)}: q\mapsto D(q\parallel p)
\end{equation*}
to second order, we get
\begin{equation*}
    D_p^{(R)}(q)=\frac{1}{2}g_{ij}d\xi^id\xi^j + O(d\xi^2) \quad\text{and}\quad D_p^{(L)}(q)=\frac{1}{2}g_{ij}d\xi^id\xi^j + O(d\xi^2)
\end{equation*}
and so to the lowest order, the asymmetry is not present.

Finally, if $g_{ij}^{(D)}$ is positive definite we say that $D$ is a \emph{divergence}, then $\frac{1}{2}g_{ij}^{(D)}$ defines a metric tensor and so a unique Riemannian structure on the manifold. The induced squared Riemannian distance coincides at the lowest order near the diagonal with the divergence.