\chapter{The information metric}
\section{Relative entropy} \label{ch:rel-entr}
Given a statistical manifold, we may consider its tangent bundle and ask ourselves if a certain metric can be naturally defined on it. Such a metric would give rise to a Riemannian connection and consequently to a geodesic distance between elements of the manifold. For this reason, we should first find a statistical meaning to the notion of distance between probability distributions and only then try to find a metric coherent with it.

One natural way to proceed is to consider how hard it is to distinguish a probability distribution from another one by extracting some samples. More precisely let's assume that a random variable has an underlying distribution $q$ and that $N_s$ samples are extracted. Then we can consider the probability that the resulting frequencies $f_i$ of the samples correspond to the probabilities $p_i$ of another probability distribution $p$.

For simplicity, let's consider the $N=2$ case, i.e. the case of binomial distributions. Let $\mathbf{q}=(t,1-t)$ and $\mathbf{p}=(r,1-r)$ be two probability distributions. Then if $N_s$ samples are drawn with underlying probability $q$, the probability $P_{N_s}(\mathbf{p})$ that the obtained frequencies correspond to $\mathbf{p}$ is given by
\begin{equation*}
    P_{N_s}(\mathbf{p})=\binom{N_s}{N_s\cdot r}t^{\,N_s\cdot r}(1-t)^{\,N_s\cdot (1-r)}
\end{equation*}
then by assuming $r\ne 0,1$ and using Stirling's formula, we obtain the following asymptotic behavior for $N_s\to\infty$
\begin{equation}
    P_{N_s}(\mathbf{p})\sim\exp\biggl\{-N_s\biggl[r\ln\biggl(\frac{r}{t}\biggr)+(1-r)\ln\biggl(\frac{1-r}{1-t}\biggr)\biggr]\biggr\}
\end{equation}
so the probability decreases exponentially with $N_s$ times the factor in the square parenthesis. This factor only depends on the probability distributions $p$ and $q$, and one may recognize it from statistics as the \emph{relative entropy} of the two distributions. For a generic finite sample space $\mathcal{X}$, the relative entropy is defined as
\begin{equation} \label{eq:rel-entropy}
    S(p\parallel q)\coloneqq\sum_{x\in\mathcal{X}}p(x)\ln\biggl(\frac{p(x)}{q(x)}\biggr)
\end{equation}

More generally it can be shown that the following theorem holds.
\begin{theorem}[Sanov's Theorem]
    Let $\mathcal{P}$ be the set of probability distributions on a finite sample space $\mathcal{X}$ and $\mathcal{E}\subseteq\mathcal{P}$ be a closed set without isolated points. Then if $N_s$ samples are drawn with underlying probability distribution $q\in\mathcal{P}$, the probability $P_{N_s}(\mathcal{E})$ that the obtained frequencies correspond to an element in $\mathcal{E}$ has the following asymptotic behavior
    \begin{equation}
        P_{N_s}(\mathcal{E})\sim e^{-N_sS(p_*\parallel q)}\qquad\text{for}\quad N_s\to\infty
    \end{equation}
    where $p_*$ is the element of $\mathcal{E}$ for which $S(p_*\parallel q)$ is smallest.
\end{theorem}
Roughly speaking, this shows that the greater the relative entropy $S(p\parallel q)$ the faster the probability of obtaining frequencies in a small neighborhood of $p$ decreases with the number of samples drawn with underlying probability $q$. In this view, relative entropy can serve as a kind of distance between probability distributions, but with some caveats. From the definition in \cref{eq:rel-entropy} relative entropy has the following properties
\begin{gather}
    S(p\parallel q)\ge 0\quad\forall p,q\in\mathcal{P}\\
    S(p\parallel q)=0\iff p=q
\end{gather}
but it is not symmetric and it doesn't follow the triangle inequality, so it is not a metric distance. We may ask ourselves if the asymmetry is an accident of our definition of relative entropy or if it is inherent in the distinguishability of probability distributions. The latter turns out to be true, as shown by the following example.
\begin{example}
    Consider two coins, one fair and one with heads on both sides. We want to pick one and guess which one it is just by tossing it multiple times. Clearly, the game is not symmetric in the choice of the coin; in fact, if we pick the fair coin the first time we will get a tail we will be sure that we picked the fair one, while if we pick the double-head one we will only get heads but this result will always be also compatible with a fair coin that, by chance, is only giving heads.
\end{example}
This game is precisely a problem of distinguishability of probability distributions. In fact, we have the $N=2$ sample space and two probability distributions: $\mathbf{p}=(0.5,0.5)$ (the fair coin) and $\mathbf{q}=(1,0)$ (the double-head coin), so the two relative entropies are $S(p\parallel q)=\infty$ and $S(q\parallel p)=\ln 2$. If we pick the fair coin for large $N_s$ the obtained frequencies will approach $\mathbf{p}$; then we consider the probability of obtaining these frequencies if the underlying distribution was $q$. This probability is identically 0, and it is coherent with Sanov's theorem since  $S(p\parallel q)=\infty$ and so $P_{N_s}(p)\sim e^{-N_s\cdot\infty}=0$. Otherwise, if we pick the double-head coin the frequencies will always match exactly $\mathbf{q}$, and the probability of getting this if the underlying distribution was $q$ is $0.5^{N_s}$. This is coherent with Sanov's theorem since  $S(q\parallel p)=\ln 2$ and so $P_{N_s}(q)\sim e^{-N_s\cdot\ln 2}=0.5^{N_s}$.

Even though relative entropy doesn't follow triangle inequality, it does follow a generalization of the Pythagorean theorem stated as follows
\begin{theorem}[Generalized Pythagorean theorem]
    Let $\mathcal{E}\subset\mathcal{P}$ be a convex set and consider $p\in\mathcal{E}$ and $q\in\mathcal{P}\setminus\mathcal{E}$. Then
    \begin{equation} \label{eq:gen-pyth}
        S(p\parallel q)\ge S(p\parallel p_*)+S(p_*\parallel q)
    \end{equation}
    where $p_*=\min\limits_{\tilde{p}\in\partial\mathcal{E}}S(\tilde{p}\parallel q)$.
\end{theorem}
This is a generalization of the Pythagorean theorem in the sense that if it was stated in terms of the Euclidean distance squared, the angle between $\overline{pp_*}$ and $\overline{p_*q}$ would be convex and so \cref{eq:gen-pyth} would be the corollary of the Pythagorean theorem for convex triangles. This suggests that the relative entropy may be regarded as an asymmetric distance squared and as we will see this is enough to define a metric on the manifold.

\section{Squared Riemannian distances} \label{ch:square-dist} \todo{Find better title: maybe Metric Riemannian manifolds?}
Now that we have some notion of distance, we shall explore how to define a coherent metric on the manifold. First, we shall study this for a Riemannian distance as defined in \ref{}.

Let $(M,g)$ be a Riemannian manifold where $g$ is the metric tensor, then consider a point $p\in M$. We define the \emph{exponential map} in $p$ as follows
\begin{equation}
    \text{Exp}_p:T_pM\to M,\quad \text{Exp}_p(\mathbf{v})\coloneqq\gamma_{\mathbf{v}}(1)
\end{equation}
where $\mathbf{v}\in T_p$ and $\gamma_{\mathbf{v}}:[0,1]\to M$ is the unique geodesic tangent to $v$ in $p$, i.e. satisfying $\gamma_{\mathbf{v}}(0)=p$ and $\gamma_{\mathbf{v}}'(0)=\mathbf{v}$. In general, this map will be well-defined only from a neighborhood of the origin of $T_p$ to a neighborhood of $p$, since only locally the uniqueness of the geodesic curve is guaranteed. By eventually further restricting the neighborhood, this map will also be 1-1 since locally the geodesic curves don't cross.

Then in this neighborhood, we have the inverse of the exponential map that maps $q\mapsto\mathbf{v}_q\in T_pM$ so that $\gamma_{\mathbf{v}_q}(1)=q$. Since $\gamma_{\mathbf{v}_q}$ is the only geodesic connecting $p$ and $q$, the geodesic distance between them will be
\begin{equation}
    L=\int_{0}^{1}\sqrt{g_{\gamma_{\mathbf{v}_q}(\lambda)}(\gamma_{\mathbf{v}_q}'(\lambda),\gamma_{\mathbf{v}_q}'(\lambda))}d\lambda
\end{equation}
Because $\gamma_{\mathbf{v}_q}$ is a geodesic, by definition we have that $\gamma_{\mathbf{v}_q}'(\lambda)$ is parallel transported along the curve and so
\begin{equation}
    g_{\gamma_{\mathbf{v}_q}(\lambda)}(\gamma_{\mathbf{v}_q}'(\lambda),\gamma_{\mathbf{v}_q}'(\lambda))=g_p(\mathbf{v}_q,\mathbf{v}_q)\quad\forall\lambda\in [0,1]
\end{equation}
Finally then, we get
\begin{equation} \label{eq:geo-dist}
    L(p,q)=\int_{0}^{1}\sqrt{g_p(\mathbf{v}_q,\mathbf{v}_q)}d\lambda=\sqrt{g_p(\mathbf{v}_q,\mathbf{v}_q)}=\lVert \mathbf{v}_q \rVert
\end{equation}
and so $\mathbf{\hat{v}}_q={\mathbf{v}_q}\backslash{L(p,q)}$.

Now chose a vector $\mathbf{dp}\in T_pM$ and let $q=\text{Exp}_p(\mathbf{dp})$. Then, let $\Gamma_{\hat{\mathbf{dp}}}$ be the unique geodesic curve such that $\Gamma_{\hat{\mathbf{dp}}}(0)=p$ and $\Gamma_{\hat{\mathbf{dp}}}'(0)=\mathbf{\hat{dp}}$. Clearly, this is the following reparametrization of $\gamma_{\mathbf{dp}}$
\begin{equation}
    \Gamma_{\hat{\mathbf{dp}}}(l)=\gamma_{\mathbf{dp}}\biggl(\frac{l}{\lVert \mathbf{v}_q \rVert}\biggr)=\gamma_{\mathbf{dp}}\biggl(\frac{l}{L(p,q)}\biggr)
\end{equation}
and so $q=\Gamma_{\hat{\mathbf{dp}}}(L(p,q))$.

Now let $\{x^{(i)}\}$ be a coordinate system for the neighborhood; from the Taylor expansion of $\Gamma_{\hat{\mathbf{dp}}}^{(i)}(l)$ in $l=0$ we get
\begin{equation}
    \Gamma_{\hat{\mathbf{dp}}}^{(i)}(l)=\Gamma_{\hat{\mathbf{dp}}}^{(i)}(0)+\left.\frac{d\Gamma_{\hat{\mathbf{dp}}}^{(i)}(l)}{dl}\right\rvert_{l=0}\cdot l+O(l^2)
\end{equation}
then, from $\Gamma_{\hat{\mathbf{dp}}}(0)=p$, $\Gamma_{\hat{\mathbf{dp}}}'(0)=\mathbf{\hat{dp}}$ and $q=\Gamma_{\hat{\mathbf{dp}}}(L(p,q))$ we get that
\begin{equation} \label{eq:p-dp}
    q^{(i)}=p^{(i)}+\hat{dp^i}\,L(p,q)+O(L^2(p,q))=p^{(i)}+dp^i+O(\lVert \mathbf{dp} \rVert^2)
\end{equation}
for every $q$ in the image of $\Gamma_{\hat{\mathbf{dp}}}$. Then, combining \cref{eq:geo-dist} and \cref{eq:p-dp} we get
\begin{equation}
    L^2(\{p^{(i)}\},\{p^{(i)}+dp^i\})\to L^2(p,q)=g_{ij}dp^i dp^j \quad \text{for} \quad dp^i\to 0
\end{equation}

We define $L_p^2:M\to [0,+\infty)$, $L_p^2(q)\coloneqq L^2(p,q)$ and consider its Taylor expansion in $q=p$
\begin{equation}
    L_p^2(\{p^{(i)}+dp^i\})=\frac{1}{2}\Bigl[\partial_i\partial_j L_p^2(q)\Bigr]_{q=p} dp^i dp^j + O(dp^3)
\end{equation}
where the first derivative vanishes because $p$ is a minimum of $L_p^2$. Then we get
\begin{equation}
    g_{ij}^{(p)}=\frac{1}{2}\Bigl[\partial_i\partial_j L_p^2(q)\Bigr]_{q=p}
\end{equation}
so the metric tensor in $p$ is proportional to the Hessian matrix of $L_p^2$ in $p$. This result allows us to recover the metric tensor from the squared Riemannian distance.

\section{Divergences}
As argued in the last paragraph of \cref{ch:rel-entr}, relative entropy has some properties of squared distances and so we may try to find a metric tensor coherent with it as we did in \cref{ch:square-dist}.

Let $M$ be an analytic manifold and $D(\,\cdot\parallel\cdot\,):M\times M\to[o,+\infty)$ a $C^2$ function (possibly asymmetric) satisfying
\begin{equation} \label{eq:div-def}
    D(p\parallel q)\ge 0\quad\text{and}\quad D(p\parallel q)=0\iff p=q\qquad \forall p,q\in M
\end{equation}
Then, given a coordinate system $\{\xi^{(i)}\}$ on $M$ we have that every pair of points $ (q,q')\in M\times M$ has coordinates $(\{\xi^{(i)}\},\{\xi^{(i)'}\})$ and we use the following notation for partial derivatives in one of the two terms on the diagonal $(p,p)$
\begin{gather*}
    D[\,\partial_i\parallel\cdot\,]\colon p\mapsto \left[\partial_i D(q\parallel q')\right]_{(q,q')=(p,p)}\\
    D[\,\cdot\parallel\partial_i\,]\colon p\mapsto \left[\partial_i' D(q\parallel q')\right]_{(q,q')=(p,p)}
\end{gather*}

From the fact that the diagonal $(p,p)$ is a constant surface of minima of $D$ follows that
\begin{equation} \label{eq:div-firstd}
    D[\,\partial_i\parallel\cdot\,]=D[\,\cdot\parallel\partial_i\,]\equiv 0
\end{equation}
so the diagonal is also a constant surface of the derivatives of $D$. Then by further deriving parallel to the diagonal, we get
\begin{gather*}
    (\partial_i+\partial_i') D[\,\cdot\parallel\partial_j\,]=D[\,\cdot\parallel\partial_i\partial_j\,]+D[\,\partial_i\parallel\partial_j\,]\equiv 0\\
    (\partial_j+\partial_j')D[\,\partial_i\parallel\cdot\,]=D[\,\partial_i\partial_j\parallel\cdot\,]+D[\,\partial_i\parallel\partial_j\,]\equiv 0
\end{gather*}
where we used the fact that since $D$ is $C^2$ we can swap second-order derivatives. Finally, we get
\begin{equation} \label{eq:div-secd}
    D[\,\partial_i\partial_j\parallel\cdot\,]=D[\,\cdot\parallel\partial_i\partial_j\,]=-D[\,\partial_i\parallel\partial_j\,]\eqqcolon g_{ij}^{(D)}
\end{equation}
where from the fact that the diagonal is a surface of minima, it follows that the previous expression defines a (symmetric) positive semi-definite matrix. It can further be shown that the Hessian matrix of any $C^2$ function of the manifold evaluated in a critical point is a $\left(\frac{0}{2}\right)$ tensor. From \cref{eq:div-firstd} and \cref{eq:div-secd} it follows that denoting
\begin{equation*}
    D_p^{(R)}: q\mapsto D(p\parallel q) \quad\text{and}\quad D_p^{(L)}: q\mapsto D(q\parallel p)
\end{equation*}
to second order, we get
\begin{gather*}
    D_p^{(R)}(q)=\frac{1}{2}g_{ij}d\xi^id\xi^j + O(d\xi^2) \quad\text{and}\quad D_p^{(L)}(q)=\frac{1}{2}g_{ij}d\xi^id\xi^j + O(d\xi^2)\\
    \text{where }d\xi^i \coloneqq \xi_p^{(i)}-\xi_q^{(i)}
\end{gather*}
and so to the lowest order, the asymmetry is not present.

Finally, if $g_{ij}^{(D)}$ is positive definite we say that $D$ is a \emph{divergence}, then $\frac{1}{2}g_{ij}^{(D)}$ defines a metric tensor and so a unique Riemannian structure on the manifold. The induced squared Riemannian distance coincides at the lowest order near the diagonal with the divergence.

\section{The Fisher information metric}
We now go back to the relative entropy, we report the definition given in \cref{ch:rel-entr}
\begin{equation*}
    S(p\parallel q)\coloneqq\sum_{x\in\mathcal{X}}p(x)\ln\left(\frac{p(x)}{q(x)}\right)
\end{equation*}
this is a $C^2$ function of $\mathcal{P}\times\mathcal{P}$ and it follows \cref{eq:div-def}. So for any model $\mathcal{S}$ with parameters $\{\theta^{(i)}\}$ we can define
\begin{equation}
    g_{ij}^{(S)}=S[\,\cdot\parallel\partial_i\partial_j\,]=\left.\partial_i\partial_j\sum_{x\in\mathcal{X}}p(x)\ln\left(\frac{p(x)}{q(x;\theta)}\right)\right\rvert_{q=p}
\end{equation}
for a fixed $p$ and $\partial_i=\frac{\partial}{\partial\theta^i}$. So we have
\begin{equation}
    g_{ij}^{(S)}=-\sum_{x\in\mathcal{X}}p(x)\partial_i\partial_j\ln p(x;\theta)=-\mathrm{E}_p[\partial_i\partial_j\ln p(x;\theta)]
\end{equation}
and equivalently
\begin{align}
    g_{ij}^{(S)}=\mathrm{E}_p\left[\frac{1}{p^2(x)}\partial_ip(x;\theta)\partial_jp(x;\theta)\right] & =\mathrm{E}_p[\partial_i\ln p(x;\theta) \partial_j\ln p(x;\theta)]\label{eq:fisher-exp}              \\
                                                                                                     & =\sum_{x\in\mathcal{X}}\frac{\partial_ip(x;\theta)\partial_jp(x;\theta)}{p(x)} \label{eq:fisher-mix}
\end{align}
where we used the fact that
\begin{equation}
    \sum_{x\in\mathcal{X}}\partial_i\partial_jp(x;\theta)=\partial_i\partial_j\sum_{x\in\mathcal{X}}p(x;\theta)=0
\end{equation}
Since we know that any $g_{ij}^{(D)}$ is positive semi-definite, $g_{ij}^{(S)}$ will be positive definite if and only if it is invertible. It can be easily shown that if the functions $\partial_ip(x;\theta)$ are linearly independent, then $g_{ij}^{(S)}$ is invertible and thus positive definite. Thus, relative entropy is a divergence and in fact, it is also known as \emph{Kullback-Leibler divergence} or \emph{information divergence}.
\todo{proof positive-definite?}

Finally, then, $g_{ij}^{(S)}$ defines a metric tensor and it is known as the \emph{Fisher information metric}. This is, up to a constant factor, the unique metric induced by the relative entropy, and it plays a focal role for the geometrical modeling and interpretation of statistics.

\section{The geometry of $\mathcal{P}$}
The Fisher metric defines the inner product $\langle\,\cdot\,,\,\cdot\,\rangle_p$ between tangent vectors of a point $p\in\mathcal{P}$. Let us now express this inner product through the representations we defined in \cref{ch:tgt-space}. Given two tangent vectors $X,Y\in T_p\mathcal{P}$ from \cref{eq:fisher-exp} we find that
\begin{equation}
    \langle X,Y \rangle_p=\mathrm{E}_p[X^{(e)}Y^{(e)}]
\end{equation}
while from \cref{eq:fisher-mix} we get
\begin{align}
    \langle X,Y \rangle_p & =\sum_{x\in\mathcal{X}}\frac{X^{(m)}(x)Y^{(m)}(x)}{p(x)}                                   \\
                          & =\sum_{x\in\mathcal{X}}{X^{(m)}(x)Y^{(e)}(x)}=\sum_{x\in\mathcal{X}}{X^{(e)}(x)Y^{(m)}(x)}
\end{align}
These expressions will prove to be very useful in \ref{}.

We notice that in neither representations the inner product is the Euclidean one induced by $\mathbb{R}^\mathcal{X}$ on the respective embeddings. For such a representation we would have that
\begin{equation} \label{eq:fisher-0-rep}
    \langle X,Y \rangle_p=\sum_{x\in\mathcal{X}}{X^{(0)}(x)Y^{(0)}(x)}
\end{equation}
then it's easy to guess that the embedding
\begin{equation}
    p\mapsto 2\,\sqrt{p} \eqqcolon p_{(0)}
\end{equation}
is the one whose representation of the tangent spaces
\begin{equation}
    X^{(0)}\coloneqq X(2\,\sqrt{p})=\frac{X(p)}{\sqrt{p}},\qquad X\in T_p(\mathcal{P})
\end{equation}
follows \cref{eq:fisher-0-rep}. This means that the information geometry of $\mathcal{P}$, i.e. the geometry iduced on it by the Fisher metric, is that of an $N$-dimensional round sphere since
\begin{equation}
    \sum_{x\in\mathcal{X}}p(x)=1 \implies \sum_{x\in\mathcal{X}}p_{(0)}^2(x)=4
\end{equation}